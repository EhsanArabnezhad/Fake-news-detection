{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8a4932",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "from scipy import sparse\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import OrderedDict\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "import tldextract\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6eb865",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANY_URL_REGEX = re.compile(r\"\"\"(?i)\\b((?:https?:(?:/{1,3}|[a-z0-9%])|[a-z0-9.\\-]+[.](?:com|net|org|edu|gov|mil|aero|asia|biz|cat|coop|info|int|jobs|mobi|museum|name|post|pro|tel|travel|xxx|ac|ad|ae|af|ag|ai|al|am|an|ao|aq|ar|as|at|au|aw|ax|az|ba|bb|bd|be|bf|bg|bh|bi|bj|bm|bn|bo|br|bs|bt|bv|bw|by|bz|ca|cc|cd|cf|cg|ch|ci|ck|cl|cm|cn|co|cr|cs|cu|cv|cx|cy|cz|dd|de|dj|dk|dm|do|dz|ec|ee|eg|eh|er|es|et|eu|fi|fj|fk|fm|fo|fr|ga|gb|gd|ge|gf|gg|gh|gi|gl|gm|gn|gp|gq|gr|gs|gt|gu|gw|gy|hk|hm|hn|hr|ht|hu|id|ie|il|im|in|io|iq|ir|is|it|je|jm|jo|jp|ke|kg|kh|ki|km|kn|kp|kr|kw|ky|kz|la|lb|lc|li|lk|lr|ls|lt|lu|lv|ly|ma|mc|md|me|mg|mh|mk|ml|mm|mn|mo|mp|mq|mr|ms|mt|mu|mv|mw|mx|my|mz|na|nc|ne|nf|ng|ni|nl|no|np|nr|nu|nz|om|pa|pe|pf|pg|ph|pk|pl|pm|pn|pr|ps|pt|pw|py|qa|re|ro|rs|ru|rw|sa|sb|sc|sd|se|sg|sh|si|sj|Ja|sk|sl|sm|sn|so|sr|ss|st|su|sv|sx|sy|sz|tc|td|tf|tg|th|tj|tk|tl|tm|tn|to|tp|tr|tt|tv|tw|tz|ua|ug|uk|us|uy|uz|va|vc|ve|vg|vi|vn|vu|wf|ws|ye|yt|yu|za|zm|zw)/)(?:[^\\s()<>{}\\[\\]]+|\\([^\\s()]*?\\([^\\s()]+\\)[^\\s()]*?\\)|\\([^\\s]+?\\))+(?:\\([^\\s()]*?\\([^\\s()]+\\)[^\\s()]*?\\)|\\([^\\s]+?\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’])|(?:(?<!@)[a-z0-9]+(?:[.\\-][a-z0-9]+)*[.](?:com|net|org|edu|gov|mil|aero|asia|biz|cat|coop|info|int|jobs|mobi|museum|name|post|pro|tel|travel|xxx|ac|ad|ae|af|ag|ai|al|am|an|ao|aq|ar|as|at|au|aw|ax|az|ba|bb|bd|be|bf|bg|bh|bi|bj|bm|bn|bo|br|bs|bt|bv|bw|by|bz|ca|cc|cd|cf|cg|ch|ci|ck|cl|cm|cn|co|cr|cs|cu|cv|cx|cy|cz|dd|de|dj|dk|dm|do|dz|ec|ee|eg|eh|er|es|et|eu|fi|fj|fk|fm|fo|fr|ga|gb|gd|ge|gf|gg|gh|gi|gl|gm|gn|gp|gq|gr|gs|gt|gu|gw|gy|hk|hm|hn|hr|ht|hu|id|ie|il|im|in|io|iq|ir|is|it|je|jm|jo|jp|ke|kg|kh|ki|km|kn|kp|kr|kw|ky|kz|la|lb|lc|li|lk|lr|ls|lt|lu|lv|ly|ma|mc|md|me|mg|mh|mk|ml|mm|mn|mo|mp|mq|mr|ms|mt|mu|mv|mw|mx|my|mz|na|nc|ne|nf|ng|ni|nl|no|np|nr|nu|nz|om|pa|pe|pf|pg|ph|pk|pl|pm|pn|pr|ps|pt|pw|py|qa|re|ro|rs|ru|rw|sa|sb|sc|sd|se|sg|sh|si|sj|Ja|sk|sl|sm|sn|so|sr|ss|st|su|sv|sx|sy|sz|tc|td|tf|tg|th|tj|tk|tl|tm|tn|to|tp|tr|tt|tv|tw|tz|ua|ug|uk|us|uy|uz|va|vc|ve|vg|vi|vn|vu|wf|ws|ye|yt|yu|za|zm|zw)\\b/?(?!@)))\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9653b75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from numpy import array\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Input, Bidirectional, LSTM, concatenate, Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import TextVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f81f30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = pd.read_csv(\"troll.csv\")\n",
    "data2 = pd.read_csv(\"regular.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20a3162",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.post.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e20ba3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1['tag'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb7ef8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc311e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1['subreddit'] = data1['link_permalink'].str.split('/').apply(lambda x: x[4])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00316f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "tldextract.extract('https://www.reddit.com/r/AskReddit/comments/85s8is/time_to_get_controversial_which_stereotypes_do/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca25443",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1['subreddit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f9b432",
   "metadata": {},
   "outputs": [],
   "source": [
    "data2['tag'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55729256",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1['text'] = data1['post'] + ' ' + data1['link_title'] + ' ' + data1['subreddit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2178bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f79cf7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.drop(['link_permalink', 'post', 'link_title', 'subreddit'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c712a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c7cd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "data2['text'] = data2['post'] + ' ' + data2['subreddit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bb60a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data2.drop(['post', 'subreddit'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4b8a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ac27d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([data1.copy(),data2.copy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3233de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7064041e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['url_1'] = data['text'].str.extract(ANY_URL_REGEX, expand=False)\n",
    "data['url_2'] = data['text'].str.extract(ANY_URL_REGEX, expand=False).apply(lambda url: tldextract.extract(url).domain if pd.notnull(url) else '')\n",
    "data['text'] = data.apply(lambda x: str(x['text']).replace(str(x['url_1']),' url '), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10003cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####  REMOVING ALL NUMBERS (HOWEVER THERE SHOULD BE BETTER THING TO DO)\n",
    "data['text'].replace(to_replace=r'[0-9]*',value=r'', regex=True, inplace=True)\n",
    "######  REMOVING SPECIAL CHARACTERS \n",
    "data['text'].replace(to_replace=r'[^A-Za-z ]',value=r' ', regex=True, inplace=True)\n",
    "######  REMOVING ALL SINGLE LETTERS\n",
    "data['text'].replace(to_replace=r'\\b\\w\\b',value=r'', regex=True, inplace=True)\n",
    "######  REMOVING MULTIPLE SPACES WITH SINGLE SPACE\n",
    "data['text'].replace(to_replace=r'\\s+',value=r' ', regex=True, inplace=True)\n",
    "data['text'] = data['text'].fillna('')\n",
    "######  REMOVING STOP WORDS\n",
    "data['text'] = data['text'].apply(lambda x: ' '.join([word for word in str(x).split() if word not in stop_words]))\n",
    "######  REMOVING FIRST SPACE FROM CELL\n",
    "data['text'].replace(to_replace=r'^ ',value=r'', regex=True, inplace=True)\n",
    "###### REMOVING \\n\n",
    "data['text'].replace(to_replace=r'\\n',value=r' newline ', regex=True, inplace=True)\n",
    "data['text'] = data['text'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c37aebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "(data.values[-2].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21bf093",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74dbdcd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag = data['tag'].values\n",
    "tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3888dc15",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tr, data_val, category_tr, category_val = train_test_split(data['text'], tag, test_size=0.2, random_state=123)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1b8135",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f4d93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(category_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8f89fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "max(data['text'].str.split().str.len())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b462e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = set()\n",
    "data['text'].str.lower().str.split().apply(results.update)\n",
    "print(len(results))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1485fc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "(data['text'].str.split().str.len()).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863a7430",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 20\n",
    "embedding_size = 25\n",
    "input_dim = 63000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0f91a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize_layer = TextVectorization(\n",
    "    #standardize=custom_standardization,\n",
    "    max_tokens=input_dim,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=seq_length)\n",
    "\n",
    "# Make a text-only dataset (no labels) and call adapt to build the vocabulary.\n",
    "#text_ds = train_ds.map(lambda x, y: x)\n",
    "vectorize_layer.adapt(data['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a024b3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vectorize_layer.get_vocabulary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9506a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize_layer.get_vocabulary()[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af7f410",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_input = Input(shape=(1,), dtype='string')\n",
    "x = vectorize_layer(nlp_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd9bc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e743bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = Embedding(output_dim=embedding_size, input_dim=input_dim, input_length=seq_length)(x)\n",
    "#emb = Dropout(0.3)(emb)\n",
    "#emb = Bidirectional(LSTM(10, return_sequences=True))(emb)\n",
    "#emb = Bidirectional(LSTM(64, return_sequences=True))(emb)\n",
    "nlp_out = Bidirectional(LSTM(embedding_size, dropout=0.3, recurrent_dropout=0.3))(emb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a55a799",
   "metadata": {},
   "outputs": [],
   "source": [
    "x= Dropout(.2)(nlp_out)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "x= Dropout(.2)(x)\n",
    "x = Dense(16, activation='relu')(x)\n",
    "x = Dense(1, activation='softmax')(x)\n",
    "model = Model(inputs=nlp_input, outputs=[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474adf21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def roc_auc(predictions,target):\n",
    "    '''\n",
    "    This methods returns the AUC Score when given the Predictions\n",
    "    and Labels\n",
    "    '''\n",
    "    \n",
    "    fpr, tpr, thresholds = metrics.roc_curve(target, predictions)\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "    return roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153233f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ef3335",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bff901",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e858915",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs\")\n",
    "history = model.fit([data_tr], category_tr, \n",
    "          batch_size=32, \n",
    "          epochs=5, \n",
    "          validation_data=([data_val], category_val), \n",
    "          callbacks=[tensorboard_callback]\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab7e082",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
