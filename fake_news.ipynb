{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a8a4932",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "from scipy import sparse\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import OrderedDict\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "import tldextract\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a6eb865",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANY_URL_REGEX = re.compile(r\"\"\"(?i)\\b((?:https?:(?:/{1,3}|[a-z0-9%])|[a-z0-9.\\-]+[.](?:com|net|org|edu|gov|mil|aero|asia|biz|cat|coop|info|int|jobs|mobi|museum|name|post|pro|tel|travel|xxx|ac|ad|ae|af|ag|ai|al|am|an|ao|aq|ar|as|at|au|aw|ax|az|ba|bb|bd|be|bf|bg|bh|bi|bj|bm|bn|bo|br|bs|bt|bv|bw|by|bz|ca|cc|cd|cf|cg|ch|ci|ck|cl|cm|cn|co|cr|cs|cu|cv|cx|cy|cz|dd|de|dj|dk|dm|do|dz|ec|ee|eg|eh|er|es|et|eu|fi|fj|fk|fm|fo|fr|ga|gb|gd|ge|gf|gg|gh|gi|gl|gm|gn|gp|gq|gr|gs|gt|gu|gw|gy|hk|hm|hn|hr|ht|hu|id|ie|il|im|in|io|iq|ir|is|it|je|jm|jo|jp|ke|kg|kh|ki|km|kn|kp|kr|kw|ky|kz|la|lb|lc|li|lk|lr|ls|lt|lu|lv|ly|ma|mc|md|me|mg|mh|mk|ml|mm|mn|mo|mp|mq|mr|ms|mt|mu|mv|mw|mx|my|mz|na|nc|ne|nf|ng|ni|nl|no|np|nr|nu|nz|om|pa|pe|pf|pg|ph|pk|pl|pm|pn|pr|ps|pt|pw|py|qa|re|ro|rs|ru|rw|sa|sb|sc|sd|se|sg|sh|si|sj|Ja|sk|sl|sm|sn|so|sr|ss|st|su|sv|sx|sy|sz|tc|td|tf|tg|th|tj|tk|tl|tm|tn|to|tp|tr|tt|tv|tw|tz|ua|ug|uk|us|uy|uz|va|vc|ve|vg|vi|vn|vu|wf|ws|ye|yt|yu|za|zm|zw)/)(?:[^\\s()<>{}\\[\\]]+|\\([^\\s()]*?\\([^\\s()]+\\)[^\\s()]*?\\)|\\([^\\s]+?\\))+(?:\\([^\\s()]*?\\([^\\s()]+\\)[^\\s()]*?\\)|\\([^\\s]+?\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’])|(?:(?<!@)[a-z0-9]+(?:[.\\-][a-z0-9]+)*[.](?:com|net|org|edu|gov|mil|aero|asia|biz|cat|coop|info|int|jobs|mobi|museum|name|post|pro|tel|travel|xxx|ac|ad|ae|af|ag|ai|al|am|an|ao|aq|ar|as|at|au|aw|ax|az|ba|bb|bd|be|bf|bg|bh|bi|bj|bm|bn|bo|br|bs|bt|bv|bw|by|bz|ca|cc|cd|cf|cg|ch|ci|ck|cl|cm|cn|co|cr|cs|cu|cv|cx|cy|cz|dd|de|dj|dk|dm|do|dz|ec|ee|eg|eh|er|es|et|eu|fi|fj|fk|fm|fo|fr|ga|gb|gd|ge|gf|gg|gh|gi|gl|gm|gn|gp|gq|gr|gs|gt|gu|gw|gy|hk|hm|hn|hr|ht|hu|id|ie|il|im|in|io|iq|ir|is|it|je|jm|jo|jp|ke|kg|kh|ki|km|kn|kp|kr|kw|ky|kz|la|lb|lc|li|lk|lr|ls|lt|lu|lv|ly|ma|mc|md|me|mg|mh|mk|ml|mm|mn|mo|mp|mq|mr|ms|mt|mu|mv|mw|mx|my|mz|na|nc|ne|nf|ng|ni|nl|no|np|nr|nu|nz|om|pa|pe|pf|pg|ph|pk|pl|pm|pn|pr|ps|pt|pw|py|qa|re|ro|rs|ru|rw|sa|sb|sc|sd|se|sg|sh|si|sj|Ja|sk|sl|sm|sn|so|sr|ss|st|su|sv|sx|sy|sz|tc|td|tf|tg|th|tj|tk|tl|tm|tn|to|tp|tr|tt|tv|tw|tz|ua|ug|uk|us|uy|uz|va|vc|ve|vg|vi|vn|vu|wf|ws|ye|yt|yu|za|zm|zw)\\b/?(?!@)))\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9653b75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "#from keras import backend as K\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "from numpy import array\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Input, Bidirectional, LSTM, concatenate, Dropout\n",
    "from tensorflow.keras.layers import Flatten\n",
    "\n",
    "#from tensorflow.keras.layers.embeddings import Embedding\n",
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f81f30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = pd.read_csv(\"troll.csv\")\n",
    "data2 = pd.read_csv(\"regular.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20a3162",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.post.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e20ba3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1['tag'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc311e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1['subreddit'] = data1['link_permalink'].str.split('/').apply(lambda x: x[4])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00316f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "tldextract.extract('https://www.reddit.com/r/AskReddit/comments/85s8is/time_to_get_controversial_which_stereotypes_do/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca25443",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79f9b432",
   "metadata": {},
   "outputs": [],
   "source": [
    "data2['tag'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55729256",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1['text'] = data1['post'] + ' ' + data1['link_title'] + ' ' + data1['subreddit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2178bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f79cf7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.drop(['link_permalink', 'post', 'link_title', 'subreddit'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c712a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c7cd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "data2['text'] = data2['post'] + ' ' + data2['subreddit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bb60a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data2.drop(['post', 'subreddit'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4b8a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88ac27d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([data1.copy(),data2.copy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3233de3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post</th>\n",
       "      <th>link_permalink</th>\n",
       "      <th>link_title</th>\n",
       "      <th>tag</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A hard look at training and tactics\" = They wi...</td>\n",
       "      <td>https://www.reddit.com/r/Bad_Cop_No_Donut/comm...</td>\n",
       "      <td>Chicago police may have violated policy in fat...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>They deserve all of the hate</td>\n",
       "      <td>https://www.reddit.com/r/Bad_Cop_No_Donut/comm...</td>\n",
       "      <td>Arkansas police refuse to return medals secret...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I guess that's what they mean when say \"I don'...</td>\n",
       "      <td>https://www.reddit.com/r/Bad_Cop_No_Donut/comm...</td>\n",
       "      <td>No Charges for Cops Who ‘Accidentally’ Fired 1...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>It's never too late for them, It's never too c...</td>\n",
       "      <td>https://www.reddit.com/r/Bad_Cop_No_Donut/comm...</td>\n",
       "      <td>Police Officer Shatters Woman's Window During ...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://petitions.whitehouse.gov//petition/pet...</td>\n",
       "      <td>https://www.reddit.com/r/Good_Cop_Free_Donut/c...</td>\n",
       "      <td>You Can Be In Favor Of Policing Reform Without...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45127</th>\n",
       "      <td>Chip Caray</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>baseball</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45128</th>\n",
       "      <td>No, it's still pay to win. I have three option...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>Paladins</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45129</th>\n",
       "      <td>THE GAP IS CLOSING, am I right?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>leagueoflegends</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45130</th>\n",
       "      <td>They were never charged through the criminal c...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>TwoXChromosomes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45131</th>\n",
       "      <td>Pretty sure this gets asked at least weekly.\\n...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>vinyl</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>51843 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    post  \\\n",
       "0      A hard look at training and tactics\" = They wi...   \n",
       "1                           They deserve all of the hate   \n",
       "2      I guess that's what they mean when say \"I don'...   \n",
       "3      It's never too late for them, It's never too c...   \n",
       "4      https://petitions.whitehouse.gov//petition/pet...   \n",
       "...                                                  ...   \n",
       "45127                                         Chip Caray   \n",
       "45128  No, it's still pay to win. I have three option...   \n",
       "45129                    THE GAP IS CLOSING, am I right?   \n",
       "45130  They were never charged through the criminal c...   \n",
       "45131  Pretty sure this gets asked at least weekly.\\n...   \n",
       "\n",
       "                                          link_permalink  \\\n",
       "0      https://www.reddit.com/r/Bad_Cop_No_Donut/comm...   \n",
       "1      https://www.reddit.com/r/Bad_Cop_No_Donut/comm...   \n",
       "2      https://www.reddit.com/r/Bad_Cop_No_Donut/comm...   \n",
       "3      https://www.reddit.com/r/Bad_Cop_No_Donut/comm...   \n",
       "4      https://www.reddit.com/r/Good_Cop_Free_Donut/c...   \n",
       "...                                                  ...   \n",
       "45127                                                NaN   \n",
       "45128                                                NaN   \n",
       "45129                                                NaN   \n",
       "45130                                                NaN   \n",
       "45131                                                NaN   \n",
       "\n",
       "                                              link_title  tag        subreddit  \n",
       "0      Chicago police may have violated policy in fat...    1              NaN  \n",
       "1      Arkansas police refuse to return medals secret...    1              NaN  \n",
       "2      No Charges for Cops Who ‘Accidentally’ Fired 1...    1              NaN  \n",
       "3      Police Officer Shatters Woman's Window During ...    1              NaN  \n",
       "4      You Can Be In Favor Of Policing Reform Without...    1              NaN  \n",
       "...                                                  ...  ...              ...  \n",
       "45127                                                NaN    0         baseball  \n",
       "45128                                                NaN    0         Paladins  \n",
       "45129                                                NaN    0  leagueoflegends  \n",
       "45130                                                NaN    0  TwoXChromosomes  \n",
       "45131                                                NaN    0            vinyl  \n",
       "\n",
       "[51843 rows x 5 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7064041e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['url_1'] = data['text'].str.extract(ANY_URL_REGEX, expand=False)\n",
    "data['url_2'] = data['text'].str.extract(ANY_URL_REGEX, expand=False).apply(lambda url: tldextract.extract(url).domain if pd.notnull(url) else '')\n",
    "data['text'] = data.apply(lambda x: str(x['text']).replace(str(x['url_1']),' url '), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10003cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####  REMOVING ALL NUMBERS (HOWEVER THERE SHOULD BE BETTER THING TO DO)\n",
    "data['text'].replace(to_replace=r'[0-9]*',value=r'', regex=True, inplace=True)\n",
    "######  REMOVING SPECIAL CHARACTERS \n",
    "data['text'].replace(to_replace=r'[^A-Za-z ]',value=r' ', regex=True, inplace=True)\n",
    "######  REMOVING ALL SINGLE LETTERS\n",
    "data['text'].replace(to_replace=r'\\b\\w\\b',value=r'', regex=True, inplace=True)\n",
    "######  REMOVING MULTIPLE SPACES WITH SINGLE SPACE\n",
    "data['text'].replace(to_replace=r'\\s+',value=r' ', regex=True, inplace=True)\n",
    "data['text'] = data['text'].fillna('')\n",
    "######  REMOVING STOP WORDS\n",
    "data['text'] = data['text'].apply(lambda x: ' '.join([word for word in str(x).split() if word not in stop_words]))\n",
    "######  REMOVING FIRST SPACE FROM CELL\n",
    "data['text'].replace(to_replace=r'^ ',value=r'', regex=True, inplace=True)\n",
    "###### REMOVING \\n\n",
    "data['text'].replace(to_replace=r'\\n',value=r' newline ', regex=True, inplace=True)\n",
    "data['text'] = data['text'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c37aebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "(data.values[-2].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21bf093",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f496b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data['text'].apply(len).plot.hist(bins=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "74dbdcd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag = data['tag'].values\n",
    "tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "30a35e17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post</th>\n",
       "      <th>link_permalink</th>\n",
       "      <th>link_title</th>\n",
       "      <th>tag</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[A, hard, look, training, tactic, '', =, They,...</td>\n",
       "      <td>https://www.reddit.com/r/Bad_Cop_No_Donut/comm...</td>\n",
       "      <td>Chicago police may have violated policy in fat...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[They, deserve, hate]</td>\n",
       "      <td>https://www.reddit.com/r/Bad_Cop_No_Donut/comm...</td>\n",
       "      <td>Arkansas police refuse to return medals secret...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[I, guess, 's, mean, say, ``, I, n't, see, col...</td>\n",
       "      <td>https://www.reddit.com/r/Bad_Cop_No_Donut/comm...</td>\n",
       "      <td>No Charges for Cops Who ‘Accidentally’ Fired 1...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[It, 's, never, late, ,, It, 's, never, cruel,...</td>\n",
       "      <td>https://www.reddit.com/r/Bad_Cop_No_Donut/comm...</td>\n",
       "      <td>Police Officer Shatters Woman's Window During ...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[http, :, //petitions.whitehouse.gov//petition...</td>\n",
       "      <td>https://www.reddit.com/r/Good_Cop_Free_Donut/c...</td>\n",
       "      <td>You Can Be In Favor Of Policing Reform Without...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                post  \\\n",
       "0  [A, hard, look, training, tactic, '', =, They,...   \n",
       "1                              [They, deserve, hate]   \n",
       "2  [I, guess, 's, mean, say, ``, I, n't, see, col...   \n",
       "3  [It, 's, never, late, ,, It, 's, never, cruel,...   \n",
       "4  [http, :, //petitions.whitehouse.gov//petition...   \n",
       "\n",
       "                                      link_permalink  \\\n",
       "0  https://www.reddit.com/r/Bad_Cop_No_Donut/comm...   \n",
       "1  https://www.reddit.com/r/Bad_Cop_No_Donut/comm...   \n",
       "2  https://www.reddit.com/r/Bad_Cop_No_Donut/comm...   \n",
       "3  https://www.reddit.com/r/Bad_Cop_No_Donut/comm...   \n",
       "4  https://www.reddit.com/r/Good_Cop_Free_Donut/c...   \n",
       "\n",
       "                                          link_title  tag subreddit  \n",
       "0  Chicago police may have violated policy in fat...    1       NaN  \n",
       "1  Arkansas police refuse to return medals secret...    1       NaN  \n",
       "2  No Charges for Cops Who ‘Accidentally’ Fired 1...    1       NaN  \n",
       "3  Police Officer Shatters Woman's Window During ...    1       NaN  \n",
       "4  You Can Be In Favor Of Policing Reform Without...    1       NaN  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import nltk preprocessing library to convert text into a readable format\n",
    "import nltk\n",
    "#nltk.download()\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('stopwords')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('stopwords')\n",
    "\n",
    "#Tokenize the string (create a list -> each index is a word)\n",
    "data['text'] = data.apply(lambda row: nltk.word_tokenize(row['text']), axis=1)\n",
    "\n",
    "#Define text lemmatization model (eg: walks will be changed to walk)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "#Loop through title dataframe and lemmatize each word\n",
    "def lemma(data):\n",
    "    return [lemmatizer.lemmatize(w) for w in data]\n",
    "\n",
    "#Apply to dataframe\n",
    "data['text'] = data['text'].apply(lemma)\n",
    "\n",
    "#Define all stopwords in the English language (it, was, for, etc.)\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "#Remove them from our dataframe\n",
    "data['text'] = data['text'].apply(lambda x: [i for i in x if i not in stop])\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3888dc15",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tr, data_val, category_tr, category_val = train_test_split(data[:10000]['text'], tag[:10000], test_size=0.3, random_state=123)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1d1b8135",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3144           [4000, VET, token, reward, ., Not, bad, .]\n",
       "3228    [Here, go, !, [, Maruku, 's, Pixiv, ], (, http...\n",
       "1214                                      [[, deleted, ]]\n",
       "309     [10, million, ?, You, 're, like, one, guy, see...\n",
       "2704    [That, 's, really, point, ,, I, mean, I, get, ...\n",
       "                              ...                        \n",
       "3074    [But, pull, whatever, absurdly, expensive, imp...\n",
       "1052    [&, gt, ;, n't, EVERY, European, power, time, ...\n",
       "5218                     [Roger, need, acting, classes.﻿]\n",
       "1346         [Geez, ,, I, wish, life, wa, simple, ∩.∩, ;]\n",
       "3582    [They, new, murderer, blame, stupidly, poor, w...\n",
       "Name: post, Length: 7000, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2cc0cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bert_preprocess_model = hub.load('https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "11972be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the bert preprocesser and bert encoder from tensorflow_hub \n",
    "import tensorflow_text\n",
    "import tensorflow_addons.text as text\n",
    "bert_preprocess = hub.load('https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1')\n",
    "bert_encoder = hub.load('https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3')\n",
    "\n",
    "bert_preprocess = hub.Module(\"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\") \n",
    "bert_encoder = hub.KerasLayer('https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4')\n",
    "\n",
    "# Input Layers\n",
    "input_layer = tf.keras.layers.Input(shape=(), dtype=tf.string, name='news')\n",
    "\n",
    "# BERT layers\n",
    "processed = bert_preprocess(input_layer)\n",
    "output = bert_encoder(processed)\n",
    "\n",
    "# Fully Connected Layers\n",
    "layer = tf.keras.layers.Dropout(0.2, name='dropout')(output['pooled_output'])\n",
    "layer = tf.keras.layers.Dense(10,activation='relu', name='hidden')(layer)\n",
    "layer = tf.keras.layers.Dense(1,activation='sigmoid', name='output')(layer)\n",
    "\n",
    "\n",
    "model = tf.keras.Model(inputs=[input_layer],outputs=[layer])\n",
    "\n",
    "#Compile model on adam optimizer, binary_crossentropy loss, and accuracy metrics\n",
    "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "#Train model on 5 epochs\n",
    "model.fit(data_tr, category_tr, epochs= 5)\n",
    "\n",
    "#Evaluate model on test data\n",
    "model.evaluate(data_val,category_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fc5d881",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fd182e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = hub.load(\"https://tfhub.dev/google/Wiki-words-250/2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cb10f1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert each series of words to a word2vec embedding\n",
    "indiv = []\n",
    "for i in data_tr:\n",
    "    temp = np.array(embed(i))\n",
    " \n",
    "    indiv.append(temp)\n",
    "    \n",
    "valdiv = []\n",
    "for i in data_val:\n",
    "    temp = np.array(embed(i))\n",
    " \n",
    "    valdiv.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "12e4acdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7000, 1761, 250)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Accounts for different length of words\n",
    "indiv = tf.keras.preprocessing.sequence.pad_sequences(indiv,dtype='float')\n",
    "valdiv = tf.keras.preprocessing.sequence.pad_sequences(valdiv,dtype='float')\n",
    "indiv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "20835827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "219/219 [==============================] - 353s 2s/step - loss: 0.6605 - accuracy: 0.6454 - val_loss: 0.6293 - val_accuracy: 0.6743\n",
      "Epoch 2/5\n",
      "219/219 [==============================] - 305s 1s/step - loss: 0.6042 - accuracy: 0.6794 - val_loss: 0.5916 - val_accuracy: 0.6940\n",
      "Epoch 3/5\n",
      "219/219 [==============================] - 308s 1s/step - loss: 0.5814 - accuracy: 0.7066 - val_loss: 0.5779 - val_accuracy: 0.7043\n",
      "Epoch 4/5\n",
      "219/219 [==============================] - 286s 1s/step - loss: 0.5702 - accuracy: 0.7121 - val_loss: 0.5717 - val_accuracy: 0.7077\n",
      "Epoch 5/5\n",
      "219/219 [==============================] - 279s 1s/step - loss: 0.5611 - accuracy: 0.7180 - val_loss: 0.5652 - val_accuracy: 0.7110\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1657fddd8>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sequential model has a 50 cell LSTM layer before Dense layers\n",
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "\n",
    "model.add(tf.keras.layers.LSTM(50))\n",
    "model.add(tf.keras.layers.Dense(20,activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(5,activation='relu'))\n",
    "\n",
    "model.add(tf.keras.layers.Dense(1,activation='sigmoid'))\n",
    "\n",
    "#Compile model with binary_crossentropy loss, Adam optimizer, and accuracy metrics\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate = 1e-4),\n",
    "              loss=\"binary_crossentropy\", metrics=['accuracy'])\n",
    "\n",
    "#Train model on 10 epochs\n",
    "model.fit(indiv, category_tr,validation_data=(valdiv,category_val), epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf3621e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b7a546",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df0dfb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe122377",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea87c0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bfe1f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fa93ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57a47cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ae559844",
   "metadata": {},
   "source": [
    "# Try and Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8f89fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#max(data['text'].str.split().str.len())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b462e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = set()\n",
    "data['text'].str.lower().str.split().apply(results.update)\n",
    "print(len(results))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1485fc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "(data['text'].str.split().str.len()).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863a7430",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 20\n",
    "embedding_size = 25\n",
    "input_dim = 63000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0f91a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize_layer = TextVectorization(\n",
    "    #standardize=custom_standardization,\n",
    "    max_tokens=input_dim,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=seq_length)\n",
    "\n",
    "# Make a text-only dataset (no labels) and call adapt to build the vocabulary.\n",
    "#text_ds = train_ds.map(lambda x, y: x)\n",
    "vectorize_layer.adapt(data['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a024b3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vectorize_layer.get_vocabulary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9506a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize_layer.get_vocabulary()[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af7f410",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_input = Input(shape=(1,), dtype='string')\n",
    "x = vectorize_layer(nlp_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd9bc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e743bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = Embedding(output_dim=embedding_size, input_dim=input_dim, input_length=seq_length)(x)\n",
    "#emb = Dropout(0.3)(emb)\n",
    "#emb = Bidirectional(LSTM(10, return_sequences=True))(emb)\n",
    "#emb = Bidirectional(LSTM(64, return_sequences=True))(emb)\n",
    "nlp_out = Bidirectional(LSTM(embedding_size, dropout=0.3, recurrent_dropout=0.3))(emb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a55a799",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Dropout(.2)(nlp_out)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "x = Dropout(.2)(x)\n",
    "x = Dense(16, activation='relu')(x)\n",
    "x = Dense(1, activation='softmax')(x)\n",
    "model = Model(inputs=nlp_input, outputs=[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474adf21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def roc_auc(predictions,target):\n",
    "    '''\n",
    "    This methods returns the AUC Score when given the Predictions\n",
    "    and Labels\n",
    "    '''\n",
    "    \n",
    "    fpr, tpr, thresholds = metrics.roc_curve(target, predictions)\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "    return roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153233f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ef3335",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bff901",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e858915",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs\")\n",
    "history = model.fit([data_tr], category_tr, \n",
    "          batch_size=32, \n",
    "          epochs=5, \n",
    "          validation_data=([data_val], category_val), \n",
    "          callbacks=[tensorboard_callback]\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab7e082",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
